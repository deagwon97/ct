{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dacon/Dacon/bdg/ct/segment_abdomen/src\n"
     ]
    }
   ],
   "source": [
    "cd ../src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.optim import lr_scheduler\n",
    "import segmentation_models_pytorch as smp\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.datasets import CT_Dataset\n",
    "from utils.utils import cal_epoch_score, figure_to_array\n",
    "from utils.report import make_report\n",
    "from metrix.metrix import Multi_Scores, cal_jaccard, cal_dice, cal_tpf, cal_fpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"\n",
    "    Seeds basic parameters for reproductibility of results.\n",
    "    Args:\n",
    "        seed (int): Number of the seed.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "            'path': \"../data/normal2\",\n",
    "            'name': \"efficientnet-b0\",\n",
    "            'model': \"efficientnet-b0\",\n",
    "            'pretrain': 'imagenet',\n",
    "            'model_path': '../models/efficientnet-b0',\n",
    "            'augmentation':[\n",
    "#                                albu.Transpose(p=0.5),\n",
    "#                                albu.RandomRotate90(3),\n",
    "#                                albu.Rotate(p=1),\n",
    "#                                albu.GridDistortion(p = 0.2), # 2배\n",
    "#                                albu.GridDropout(p=0.5, ratio = 0.3),\n",
    "                           ],\n",
    "    \n",
    "            'num_epochs': 20,\n",
    "            'batch_size': 28,\n",
    "            'optimizer' : torch.optim.Adam,\n",
    "            'scheduler' : lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "            'loss' : smp.utils.losses.DiceLoss,\n",
    "            'seed' : 1015,\n",
    "            'device' : 'cuda'\n",
    "        }\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "seed_everything(CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeagwon-bu\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.20<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">radiant-flower-94</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/deagwon-bu/ct_segment\" target=\"_blank\">https://wandb.ai/deagwon-bu/ct_segment</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/deagwon-bu/ct_segment/runs/3tqcjqxg\" target=\"_blank\">https://wandb.ai/deagwon-bu/ct_segment/runs/3tqcjqxg</a><br/>\n",
       "                Run data is saved locally in <code>/home/dacon/Dacon/bdg/ct/segment_abdomen/src/wandb/run-20210225_185534-3tqcjqxg</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wandb 설정\n",
    "wandb.init(project=\"ct_segment\", config=CONFIG,  reinit = True)\n",
    "wandb.run.name = CONFIG['name']\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_paths = os.listdir(CONFIG['path'] + '/img/')\n",
    "images_paths_onlypng = []\n",
    "for file_name in images_paths:\n",
    "    if '.png' in file_name:\n",
    "        images_paths_onlypng.append(file_name)\n",
    "images_paths = images_paths_onlypng\n",
    "#images_paths.pop(-1)\n",
    "images_paths = np.array(images_paths)\n",
    "train_images = images_paths[len(images_paths) // 10 : ]\n",
    "test_images  = images_paths[:len(images_paths) // 10]\n",
    "kfold = KFold(n_splits=5, shuffle = False)\n",
    "target_fold_index = 1\n",
    "train_metrics_list = []\n",
    "valid_metrics_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/anaconda3/envs/ct/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:2247: UserWarning: Using lambda is incompatible with multiprocessing. Consider using regular functions or partial().\n",
      "  warnings.warn('Using lambda is incompatible with multiprocessing. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train:   0%|          | 0/1896 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dacon/anaconda3/envs/ct/lib/python3.7/site-packages/segmentation_models_pytorch/base/modules.py:102: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.activation(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  11%|█         | 205/1896 [04:49<38:06,  1.35s/it, dice_loss - 0.3816, avg_jaccard - 0.5723, avg_dice - 0.6924, avg_tpf - 0.7957, avg_fpf - 0.05605]"
     ]
    }
   ],
   "source": [
    "for fold_index, (trn_idx, val_idx) in enumerate(kfold.split(train_images)):\n",
    "    if fold_index == target_fold_index:\n",
    "        print(fold_index)\n",
    "        train_fold = train_images[trn_idx]\n",
    "        valid_fold = train_images[val_idx]\n",
    "        ENCODER = CONFIG['model']\n",
    "        ENCODER_WEIGHTS = CONFIG['pretrain']\n",
    "        ACTIVATION = 'softmax'\n",
    "        CLASSES = 4\n",
    "        \n",
    "        model = smp.Unet(\n",
    "            encoder_name=ENCODER, \n",
    "            encoder_weights=ENCODER_WEIGHTS, \n",
    "            in_channels = 1,\n",
    "            classes=CLASSES, \n",
    "            activation = ACTIVATION,\n",
    "        )\n",
    "        \n",
    "        #preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n",
    "        \n",
    "        \n",
    "        train_dataset = CT_Dataset(\n",
    "            path = CONFIG['path'],\n",
    "            imglist = train_fold,\n",
    "            augmentation = None,#get_training_augmentation(), \n",
    "            preprocessing = None,#get_preprocessing(preprocessing_fn),\n",
    "            classes=4,\n",
    "        )\n",
    "        \n",
    "        valid_dataset = CT_Dataset(\n",
    "            path = CONFIG['path'],\n",
    "            imglist = valid_fold,\n",
    "            augmentation = None,#get_training_augmentation(), \n",
    "            preprocessing = None,#get_preprocessing(preprocessing_fn),\n",
    "            classes=4,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        BATCH_SIZE = CONFIG['batch_size']\n",
    "        train_step_size = train_dataset.__len__() // BATCH_SIZE\n",
    "        valid_step_size = valid_dataset.__len__() // BATCH_SIZE\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "\n",
    "        #loss = smp.utils.losses.DiceLoss()\n",
    "        #loss = utils.train.WeightedDiceLoss()\n",
    "        loss = CONFIG['loss']()\n",
    "\n",
    "        train_metrics = [\n",
    "            Multi_Scores(metric = cal_jaccard, name      = 'avg_jaccard'),\n",
    "            Multi_Scores(metric = cal_dice,    name      =    'avg_dice'),\n",
    "            Multi_Scores(metric = cal_tpf,     name      =     'avg_tpf'),\n",
    "            Multi_Scores(metric = cal_fpf,     name      =     'avg_fpf')\n",
    "        ]\n",
    "        valid_metrics = [\n",
    "            Multi_Scores(metric = cal_jaccard, name      = 'avg_jaccard'),\n",
    "            Multi_Scores(metric = cal_dice,    name      =    'avg_dice'),\n",
    "            Multi_Scores(metric = cal_tpf,     name      =     'avg_tpf'),\n",
    "            Multi_Scores(metric = cal_fpf,     name      =     'avg_fpf')\n",
    "        ]\n",
    "\n",
    "        optimizer = CONFIG['optimizer']([ \n",
    "            dict(params=model.parameters(), lr=0.0001),\n",
    "        ])\n",
    "\n",
    "        #scheduler = CONFIG['scheduler'](optimizer, 10, 2, eta_min=1e-6)\n",
    "        train_epoch = smp.utils.train.TrainEpoch(\n",
    "            model, \n",
    "            loss=loss, \n",
    "            metrics=train_metrics, \n",
    "            optimizer=optimizer,\n",
    "            device=CONFIG['device'],\n",
    "            verbose=True,\n",
    "        )\n",
    "        \n",
    "        valid_epoch = smp.utils.train.ValidEpoch(\n",
    "            model, \n",
    "            loss=loss, \n",
    "            metrics=valid_metrics, \n",
    "            device=CONFIG['device'],\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        # 한 폴드당 150 Epoch 수행\n",
    "        NUM_EPOCH = CONFIG['num_epochs']\n",
    "        min_score = np.Inf\n",
    "        if not os.path.exists(CONFIG['model_path']):\n",
    "            subprocess.call([\"mkdir\",\n",
    "                         CONFIG['model_path']],\n",
    "                         shell = False)\n",
    "        #MODEL = train_name\n",
    "        patient = 0\n",
    "        max_valid_dice = 0\n",
    "        for i in range(0, NUM_EPOCH):\n",
    "            \n",
    "            print('\\nEpoch: {}'.format(i))\n",
    "            train_logs = train_epoch.run(train_loader)\n",
    "            valid_logs = valid_epoch.run(valid_loader)\n",
    "            \n",
    "\n",
    "            score_log = {}\n",
    "            \n",
    "            train_log = {}\n",
    "            for metric_idx, cal in enumerate(['jaccard', 'dice',\n",
    "                                              'tpf', 'ftp']):\n",
    "                score_dic = cal_epoch_score(train_metrics,\n",
    "                                            metric_idx,\n",
    "                                            train_step_size,\n",
    "                                            run = 'train')\n",
    "                train_log.update(score_dic)\n",
    "            valid_log = {}\n",
    "            for metric_idx, cal in enumerate(['jaccard', 'dice',\n",
    "                                              'tpf', 'ftp']):\n",
    "                score_dic = cal_epoch_score(valid_metrics,\n",
    "                                            metric_idx,\n",
    "                                            valid_step_size,\n",
    "                                            run = 'valid')\n",
    "                valid_log.update(score_dic)\n",
    "                \n",
    "            score_log.update(train_log)\n",
    "            score_log.update(valid_log)\n",
    "            wandb.log(score_log)\n",
    "            \n",
    "            #wandb.log(new_logs)\n",
    "            #scheduler.step()\n",
    "            if max_valid_dice < valid_logs['avg_dice']:\n",
    "                max_valid_dice = valid_logs['avg_dice']\n",
    "                torch.save(model, f\"{CONFIG['model_path']}/{CONFIG['model']}_{max_valid_dice:.4f}loss_{i}epochs.pth\")\n",
    "                \n",
    "                \n",
    "                make_report(train_log).to_csv(f\"{CONFIG['model_path']}/train_report.csv\")\n",
    "                make_report(valid_log).to_csv(f\"{CONFIG['model_path']}/valid_report.csv\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    sample_img, sample_mask = valid_dataset[np.random.randint(valid_dataset.__len__())]\n",
    "                    sample_tensor = torch.cuda.FloatTensor(sample_img[np.newaxis,...])\n",
    "                    sample_pred   = model.predict(sample_tensor).cpu().numpy()[0]\n",
    "\n",
    "                    plt.figure(figsize=(20,5))\n",
    "                    #img, mask = dataset[img_id]\n",
    "                    plt.subplot(1,4,1)\n",
    "                    plt.imshow(sample_img[0,:,:], cmap='bone') # 원본 # permute는 축 변경\n",
    "                    plt.gca().set_title(\"Image\")\n",
    "                    plt.subplot(1,4,2)\n",
    "                    plt.imshow(sample_img[0,:,:], cmap='bone') # 원본 # permute는 축 변경\n",
    "                    plt.imshow(sample_mask.transpose([1,2,0]).argmax(axis = 2), alpha=0.3, cmap='flag') # 레이블\n",
    "                    plt.gca().set_title(\"True Mask\")\n",
    "                    plt.subplot(1,4,3)\n",
    "                    plt.imshow(sample_img[0,:,:], cmap='bone') # 원본 # permute는 축 변경\n",
    "                    plt.imshow(sample_pred.transpose([1,2,0]).argmax(axis = 2), alpha=0.3, cmap='flag') # 레이블\n",
    "                    plt.gca().set_title(\"Predicted Mask\")\n",
    "                    plt.subplot(1,4,4)\n",
    "                    plt.imshow(sample_mask.transpose([1,2,0]).argmax(axis = 2), alpha=0.3, cmap='flag') # 레이블\n",
    "                    plt.imshow(sample_pred.transpose([1,2,0]).argmax(axis = 2), alpha=0.3, cmap='flag') # 레이블\n",
    "                    plt.gca().set_title(\"Overlay\")\n",
    "                    \n",
    "                    # save in wandb\n",
    "                    plt.tight_layout()\n",
    "                    fig = plt.gcf()\n",
    "                    figure_array = figure_to_array(fig)\n",
    "                    wandb.log({\"examples\": [wandb.Image(figure_array,\n",
    "                                                        caption=f\"{CONFIG['name']}_{max_valid_dice:.4f}loss_{i}epochs\")]})\n",
    "                    plt.show()\n",
    "                    \n",
    "            else:\n",
    "                patient += 1\n",
    "                if patient > 5:\n",
    "                    print(\"early stopping\")\n",
    "                    break\n",
    "        \n",
    "        train_metrics_list.append(train_metrics)\n",
    "        valid_metrics_list.append(valid_metrics)\n",
    "        target_fold_index += 1\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ct",
   "language": "python",
   "name": "ct"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
